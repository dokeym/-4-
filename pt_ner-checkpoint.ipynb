{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9180a466-4548-4b3e-bf6e-c3e0345eb74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/analysis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#导入必要的库\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from transformers.models.bert.modeling_bert import *\n",
    "from tqdm import tqdm\n",
    "import  NER_config\n",
    "import jsonlines\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "from loguru import logger\n",
    "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup, AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchcrf import CRF\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#生成日志\n",
    "log_path = \"/data/users/yinqa/freelance/pt_ner/\"\n",
    "logger.add(log_path + 'Train.log', format=\"{time} {level} {message}\", level=\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bfac632-0fa0-4065-b6fb-5150c9726497",
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据处理\n",
    "def Data_preprocess(input_filename,output_filename):\n",
    "    count = 0\n",
    "    word_list = []\n",
    "    label_list = []\n",
    "    with open(input_filename,'r') as reader:\n",
    "        lines = reader.readlines()\n",
    "    random_list = []\n",
    "    #选取12000条数据\n",
    "    for _ in tqdm(range(12000)):#12000\n",
    "        #设定随机值,进行随机选取\n",
    "        random_index = random.randint(1,len(lines)-1) #测试499497 #训练集4495465\n",
    "        if random_index not in random_list:\n",
    "            random_list.append(random_index)\n",
    "            json_line = json.loads(lines[random_index].strip())\n",
    "            text = json_line['text']\n",
    "            #设定了选取长度\n",
    "            if len(text) <= 510:\n",
    "                words = list(text)\n",
    "                label_entity = json_line.get('label',None)\n",
    "                #label先全部设为\"O\"\n",
    "                label = ['O'] * len(words)\n",
    "                #判断如果不等于None\n",
    "                if label_entity is not None:\n",
    "                    count += 1\n",
    "                    for key,value in label_entity.items():\n",
    "                        for sub_name,sub_index in value.items():\n",
    "                            for start_index,end_index in sub_index:\n",
    "                                #判断是否超出边界，做一个判断\n",
    "                                if ''.join(words[start_index:end_index + 1]) == sub_name: \n",
    "                                    #单实体标注S-entity\n",
    "                                    if start_index == end_index:\n",
    "                                        label[start_index] = 'S-' + key\n",
    "                                    else:\n",
    "                                        #多字实体采用B-entity I-entity E-entity 的标注方式\n",
    "                                        label[start_index] = \"B-\" + key\n",
    "                                        label[start_index + 1:end_index + 1] = ['I-' + key] * (len(sub_name) -1 )\n",
    "                                        label[end_index] = 'E-' + key \n",
    " \n",
    "                word_list.append(words)\n",
    "                label_list.append(label)\n",
    "            else:\n",
    "                continue\n",
    "    print(len(word_list),len(label_list))\n",
    "    #保存成二进制文件\n",
    "    np.savez_compressed(output_filename,words = word_list, lables = label_list)\n",
    "    #统计处理数量\n",
    "    print(count)\n",
    "    # return word_list,label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "636133c9-08d1-4ada-9a32-8053549463c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12000/12000 [00:00<00:00, 17651.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7187 7187\n",
      "7187\n"
     ]
    }
   ],
   "source": [
    "train_input = 'NER_train.json'\n",
    "train_output = 'train.npz'\n",
    "Data_preprocess(train_input,train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c904f88-6613-4e3b-8e94-cbebd4912cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12000/12000 [00:00<00:00, 93513.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1341 1341\n",
      "1341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_input = 'NER_test.json'\n",
    "test_output = 'test.npz'\n",
    "Data_preprocess(test_input,test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bed64a6-b458-4ef5-b877-c13659ac4612",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, words, labels, config, word_pad_idx=0, label_pad_idx=-1):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('/data/users/yinqa/freelance/pt_ner/chinese_roberta_wwm_large_ext/', do_lower_case=True)\n",
    "        self.label2id = config.label2id\n",
    "        self.id2label = {_id: _label for _label, _id in list(config.label2id.items())}\n",
    "        self.dataset = self.preprocess(words, labels)\n",
    "        self.word_pad_idx = word_pad_idx\n",
    "        self.label_pad_idx = label_pad_idx\n",
    "        self.device = config.device\n",
    "    def preprocess(self, origin_sentences, origin_labels):\n",
    "        data = []\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        for line in tqdm(origin_sentences):\n",
    "            # replace each token by its index\n",
    "            # we can not use encode_plus because our sentences are aligned to labels in list type\n",
    "            words = []\n",
    "            word_lens = []\n",
    "            for token in line:\n",
    "                #bert对字进行编码转化为id表示\n",
    "                words.append(self.tokenizer.tokenize(token))\n",
    "                word_lens.append(len(token))\n",
    "            # 变成单个字的列表，开头加上[CLS]\n",
    "            words = ['[CLS]'] + [item for token in words for item in token]\n",
    "            token_start_idxs = 1 + np.cumsum([0] + word_lens[:-1])\n",
    "            sentences.append((self.tokenizer.convert_tokens_to_ids(words), token_start_idxs))\n",
    "        for tag in origin_labels:\n",
    "            label_id = [self.label2id.get(t) for t in tag]\n",
    "            labels.append(label_id)\n",
    "        for sentence, label in zip(sentences, labels):\n",
    "            if len(sentence[0]) - len(label) == 1:\n",
    "                data.append((sentence, label))\n",
    "        return data\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"sample data to get batch\"\"\"\n",
    "        word = self.dataset[idx][0]\n",
    "        label = self.dataset[idx][1]\n",
    "        return [word, label]\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    " \n",
    "    def collate_fn(self, batch):\n",
    "        sentences = [x[0] for x in batch]\n",
    "        labels = [x[1] for x in batch]\n",
    "        # batch length\n",
    "        batch_len = len(sentences)\n",
    "        # compute length of longest sentence in batch\n",
    "        max_len = max([len(s[0]) for s in sentences])\n",
    "        max_label_len = 0 # 改动前max_label_len = 0\n",
    "        # padding data 初始化\n",
    "        batch_data = self.word_pad_idx * np.ones((batch_len, max_len))\n",
    "        batch_label_starts = []\n",
    "        # padding and aligning\n",
    "        for j in range(batch_len):\n",
    "            cur_len = len(sentences[j][0])\n",
    "            batch_data[j][:cur_len] = sentences[j][0]\n",
    "            # 找到有标签的数据的index（[CLS]不算）\n",
    "            label_start_idx = sentences[j][-1]\n",
    "            label_starts = np.zeros(max_len)\n",
    "            label_starts[[idx for idx in label_start_idx if idx < max_len]] = 1\n",
    "            batch_label_starts.append(label_starts)\n",
    "            max_label_len = max(int(sum(label_starts)), max_label_len)\n",
    " \n",
    "        # padding label\n",
    "        batch_labels = self.label_pad_idx * np.ones((batch_len, max_label_len))\n",
    "        for j in range(batch_len):\n",
    "            cur_tags_len = len(labels[j])\n",
    "            batch_labels[j][:cur_tags_len] = labels[j]\n",
    "        # convert data to torch LongTensors\n",
    "        batch_data = torch.tensor(batch_data, dtype=torch.long)\n",
    "        batch_label_starts = torch.tensor(batch_label_starts, dtype=torch.long)\n",
    "        batch_labels = torch.tensor(batch_labels, dtype=torch.long)\n",
    " \n",
    "        # shift tensors to GPU if available\n",
    "        batch_data, batch_label_starts = batch_data.to(self.device), batch_label_starts.to(self.device)\n",
    "        batch_labels = batch_labels.to(self.device)\n",
    "        return [batch_data, batch_label_starts, batch_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0845e124-80b2-4b4c-b215-09a35e7df4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertNER(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertNER, self).__init__(config)\n",
    "        #定义分类类别，也可以写在加载预训练模型的config文件中\n",
    "        self.num_labels = 5\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.bilstm = nn.LSTM(\n",
    "            # input_size=config.lstm_embedding_size,  # 1024\n",
    "            input_size=786,  # 1024\n",
    "            hidden_size=config.hidden_size // 2,  # 1024\n",
    "            batch_first=True,\n",
    "            num_layers=2,\n",
    "            # dropout=config.lstm_dropout_prob,  # 0.5\n",
    "            dropout=0.5,  # 0.5\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "        self.crf = CRF(self.num_labels, batch_first=True)\n",
    " \n",
    "        self.init_weights()\n",
    " \n",
    "    def forward(self, input_data, token_type_ids=None, attention_mask=None, labels=None,\n",
    "                position_ids=None, inputs_embeds=None, head_mask=None):\n",
    "        input_ids, input_token_starts = input_data\n",
    "        outputs = self.bert(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            position_ids=position_ids,\n",
    "                            head_mask=head_mask,\n",
    "                            inputs_embeds=inputs_embeds)\n",
    "        sequence_output = outputs[0]\n",
    " \n",
    "        # 去除[CLS]标签等位置，获得与label对齐的pre_label表示\n",
    "        origin_sequence_output = [layer[starts.nonzero().squeeze(1)]\n",
    "                                  for layer, starts in zip(sequence_output, input_token_starts)]\n",
    "        # 将sequence_output的pred_label维度padding到最大长度\n",
    "        padded_sequence_output = pad_sequence(origin_sequence_output, batch_first=True)\n",
    "        # dropout pred_label的一部分feature\n",
    "        padded_sequence_output = self.dropout(padded_sequence_output)\n",
    "        #将结果送入bilstm，再次提取特性        \n",
    "        lstm_output, _ = self.bilstm(padded_sequence_output)\n",
    "        # 将lstm的结果送入线性层，进行五分类\n",
    "        logits = self.classifier(lstm_output)\n",
    "        outputs = (logits,)\n",
    "        if labels is not None:\n",
    "            loss_mask = labels.gt(-1)\n",
    "            #将每个标签的概率送入到crf中进行解码，并获得loss\n",
    "            loss = self.crf(logits, labels, loss_mask) * (-1)\n",
    "            outputs = (loss,) + outputs\n",
    "        # contain: (loss), scores\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcca38c6-5706-4c2b-a92f-b0221f885f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义训练函数\n",
    "def train_epoch(train_loader, model, optimizer, scheduler, epoch):\n",
    "    # 设定训练模式\n",
    "    model.train()\n",
    "    train_losses = 0\n",
    "    for idx, batch_samples in enumerate(tqdm(train_loader)):\n",
    "        batch_data, batch_token_starts, batch_labels = batch_samples\n",
    "        batch_masks = batch_data.gt(0)  # get padding mask\n",
    "        # 计算损失值\n",
    "        loss = model((batch_data, batch_token_starts),\n",
    "                     token_type_ids=None, attention_mask=batch_masks, labels=batch_labels)[0]\n",
    "        train_losses += loss.item()\n",
    "        #梯度更新\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        # 梯度裁剪\n",
    "        nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=NER_config.clip_grad)\n",
    "        # 计算梯度\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    train_loss = float(train_losses) / len(train_loader)\n",
    "    logger.info(\"Epoch: {}, train loss: {}\",epoch, train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66825114-076f-409c-8ec8-f99e91b169e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#根据预测值和真实值计算评价指标\n",
    "def compute_acc_recall(batch_output,batch_tags):\n",
    "    acc = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    for index in range(len(batch_output)):\n",
    "        acc += accuracy_score(batch_output[index],batch_tags[index])\n",
    "        recall += recall_score(batch_output[index],batch_tags[index],average='macro')\n",
    "        f1 += f1_score(batch_output[index],batch_tags[index],average='macro')\n",
    "    return (acc/len(batch_output),recall/len(batch_output),f1/len(batch_output))\n",
    "#定义验证函数\n",
    "def evaluate(dev_loader, model, mode='dev'):\n",
    "    # 设置为模型为验证模式\n",
    "    model.eval()\n",
    "    if mode == 'test':\n",
    "        tokenizer = BertTokenizer.from_pretrained('/data/users/yinqa/freelance/pt_ner/chinese_roberta_wwm_large_ext/', do_lower_case=True, skip_special_tokens=True)\n",
    "    id2label = NER_config.id2label\n",
    "    true_tags = []\n",
    "    pred_tags = []\n",
    "    sent_data = []\n",
    "    dev_losses = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, batch_samples in tqdm(enumerate(dev_loader)):\n",
    "            batch_data, batch_token_starts, batch_tags = batch_samples\n",
    "            if mode == 'test':\n",
    "                sent_data.extend([[tokenizer.convert_ids_to_tokens(idx.item()) for idx in indices\n",
    "                                   if (idx.item() > 0 and idx.item() != 101)] for indices in batch_data])\n",
    "            batch_masks = batch_data.gt(0)  # get padding mask, gt(x): get index greater than x\n",
    "            label_masks = batch_tags.gt(-1)  # get padding mask, gt(x): get index greater than x\n",
    "            # compute model output and loss\n",
    "            loss = model((batch_data, batch_token_starts),\n",
    "                         token_type_ids=None, attention_mask=batch_masks, labels=batch_tags)[0]\n",
    "            dev_losses += loss.item()\n",
    "            # (batch_size, max_len, num_labels)\n",
    "            batch_output = model((batch_data, batch_token_starts),\n",
    "                                 token_type_ids=None, attention_mask=batch_masks)[0]\n",
    "            # (batch_size, max_len - padding_label_len)\n",
    "            batch_output = model.crf.decode(batch_output, mask=label_masks)\n",
    "            # (batch_size, max_len)\n",
    "            batch_tags = batch_tags.to('cpu').numpy()\n",
    "            pred_tags.extend([[idx for idx in indices] for indices in batch_output])\n",
    "            # (batch_size, max_len - padding_label_len)\n",
    "            true_tags.extend([[idx for idx in indices if idx > -1] for indices in batch_tags])\n",
    "            #pred_tags.extend([[id2label.get(idx) for idx in indices] for indices in batch_output])\n",
    "            # (batch_size, max_len - padding_label_len)\n",
    "            #true_tags.extend([[id2label.get(idx) for idx in indices if idx > -1] for indices in batch_tags])\n",
    "    assert len(pred_tags) == len(true_tags)\n",
    "    # logging loss, f1 and report\n",
    "    metrics = {}\n",
    "    acc , recall, F1= compute_acc_recall(true_tags,pred_tags)\n",
    "    metrics['acc'] = acc\n",
    "    metrics['recal'] = recal\n",
    "    metrics['f1'] = F1\n",
    "    metrics['loss'] = float(dev_losses) / len(dev_loader)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "833bbc5f-d0fb-481d-adf6-92fc98365f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(NER_config):\n",
    "    data = np.load(NER_config.test_dir, allow_pickle=True)\n",
    "    word_test = data[\"words\"]\n",
    "    label_test = data[\"labels\"]\n",
    "    test_dataset = NERDataset(word_test, label_test, NER_config)\n",
    "    # build data_loader\n",
    "    test_loader = DataLoader(test_dataset, batch_size=NER_config.batch_size,\n",
    "                             shuffle=False, collate_fn=test_dataset.collate_fn)\n",
    "    # Prepare model\n",
    "    if config.model_dir is not None:\n",
    "        model = BertNER.from_pretrained(NER_config.model_dir)\n",
    "        model.to(NER_config.device)\n",
    "    val_metrics = evaluate(test_loader, model, mode='test')\n",
    "    logging.info(\"test loss: {}, f1 score: {}\".format(val_metrics['loss'], val_metrics['F1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3c5cf59-a707-4033-9f2e-da4a1e02412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, dev_loader, model, optimizer, scheduler, model_dir):\n",
    "    \"\"\"train the model and test model performance\"\"\"\n",
    "    # reload weights from restore_dir if specified\n",
    "    best_val_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "    # start training\n",
    "    for epoch in range(1, NER_config.epoch_num + 1):\n",
    "        train_epoch(train_loader, model, optimizer, scheduler, epoch)\n",
    "        #开始验证\n",
    "        val_metrics = evaluate(dev_loader, model, mode='dev')\n",
    "        val_f1 = val_metrics['f1']\n",
    "        logger.info(\"Epoch: {}, dev loss: {}, f1 score: {}\",epoch, val_metrics['loss'], val_f1)\n",
    "        improve_f1 = val_f1 - best_val_f1\n",
    "        if improve_f1 > 1e-5:\n",
    "            best_val_f1 = val_f1\n",
    "            #模型保存需要更改\n",
    "            torch.save(model,model_dir)\n",
    "            logger.info(\"--------Save best model!--------\")\n",
    "            if improve_f1 < NER_config.patience:\n",
    "                patience_counter += 1\n",
    "            else:\n",
    "                patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        # Early stopping and logging best f1\n",
    "        if (patience_counter >= NER_config.patience_num and epoch > NER_config.min_epoch_num) or epoch == NER_config.epoch_num:\n",
    "            logger.info(\"Best val f1: {}\",best_val_f1)\n",
    "            break\n",
    "    logger.info(\"Training Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62d6e259-5b94-46e5-be1b-8d9ba41c0009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_split(dataset_dir):\n",
    "    \"\"\"从训练集合中划分验证集和训练集\"\"\"\n",
    "    data = np.load(dataset_dir, allow_pickle=True)\n",
    "    words = data[\"words\"]\n",
    "    labels = data[\"lables\"]\n",
    "    x_train, x_dev, y_train, y_dev = train_test_split(words, labels, test_size=0.01, random_state=0)\n",
    "    return x_train, x_dev, y_train, y_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ab8215a-0739-4dec-9a28-fa18095da098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(config):\n",
    "    \"\"\"train the model\"\"\"\n",
    "    # 处理数据，\n",
    "    # 分离训练集、验证集\n",
    "    word_train, word_dev, label_train, label_dev = dev_split('train.npz')\n",
    "    # 创建dataset\n",
    "    train_dataset = NERDataset(word_train, label_train, config)\n",
    "    dev_dataset = NERDataset(word_dev, label_dev, config)\n",
    "    # get dataset size\n",
    "    train_size = len(train_dataset)\n",
    "    # 创建dataloader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size,\n",
    "                              shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=config.batch_size,\n",
    "                            shuffle=True, collate_fn=dev_dataset.collate_fn)\n",
    "    \n",
    "    # 实例化模型\n",
    "    device = config.device\n",
    "    model = BertNER.from_pretrained('/data/users/yinqa/freelance/pt_ner/chinese_roberta_wwm_large_ext/', num_labels=len(config.label2id))\n",
    "    model.to(device)\n",
    "    # Prepare optimizer\n",
    "    if config.full_fine_tuning:\n",
    "        # model.named_parameters(): [bert, bilstm, classifier, crf]\n",
    "        bert_optimizer = list(model.bert.named_parameters())\n",
    "        lstm_optimizer = list(model.bilstm.named_parameters())\n",
    "        classifier_optimizer = list(model.classifier.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in bert_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': config.weight_decay},\n",
    "            {'params': [p for n, p in bert_optimizer if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in lstm_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'lr': config.learning_rate * 5, 'weight_decay': config.weight_decay},\n",
    "            {'params': [p for n, p in lstm_optimizer if any(nd in n for nd in no_decay)],\n",
    "             'lr': config.learning_rate * 5, 'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in classifier_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'lr': config.learning_rate * 5, 'weight_decay': config.weight_decay},\n",
    "            {'params': [p for n, p in classifier_optimizer if any(nd in n for nd in no_decay)],\n",
    "             'lr': config.learning_rate * 5, 'weight_decay': 0.0},\n",
    "            {'params': model.crf.parameters(), 'lr': config.learning_rate * 5}\n",
    "        ]\n",
    "    # only fine-tune the head classifier\n",
    "    else:\n",
    "        param_optimizer = list(model.classifier.named_parameters())\n",
    "        optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer]}]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=config.learning_rate, correct_bias=False)\n",
    "    train_steps_per_epoch = train_size // config.batch_size\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=(config.epoch_num // 10) * train_steps_per_epoch,\n",
    "                                                num_training_steps=config.epoch_num * train_steps_per_epoch)\n",
    " \n",
    "    # Train the model\n",
    "    # logging.info(\"--------Start Training!--------\")\n",
    "    train(train_loader, dev_loader, model, optimizer, scheduler, config.model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09cc9dbe-829b-4804-8583-247279a0601d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7115/7115 [00:12<00:00, 587.20it/s]\n",
      "100%|██████████| 72/72 [00:00<00:00, 588.80it/s]\n",
      "Some weights of BertNER were not initialized from the model checkpoint at /data/users/yinqa/freelance/pt_ner/chinese_roberta_wwm_large_ext/ and are newly initialized: ['bilstm.weight_ih_l1_reverse', 'bilstm.bias_ih_l0_reverse', 'crf.end_transitions', 'bilstm.weight_ih_l0', 'bilstm.bias_hh_l1_reverse', 'bilstm.bias_ih_l0', 'bilstm.weight_hh_l0_reverse', 'crf.start_transitions', 'bilstm.weight_hh_l0', 'bilstm.bias_ih_l1_reverse', 'bilstm.bias_ih_l1', 'bilstm.bias_hh_l1', 'classifier.bias', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l1', 'classifier.weight', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l1', 'bilstm.bias_hh_l0_reverse', 'bilstm.weight_hh_l1_reverse', 'crf.transitions']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/712 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[2097152, 1]' is invalid for input of size 1609728",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNER_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     test(NER_config)\n",
      "Cell \u001b[0;32mIn[11], line 55\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     49\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m get_cosine_schedule_with_warmup(optimizer,\n\u001b[1;32m     50\u001b[0m                                             num_warmup_steps\u001b[38;5;241m=\u001b[39m(config\u001b[38;5;241m.\u001b[39mepoch_num \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m*\u001b[39m train_steps_per_epoch,\n\u001b[1;32m     51\u001b[0m                                             num_training_steps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mepoch_num \u001b[38;5;241m*\u001b[39m train_steps_per_epoch)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# logging.info(\"--------Start Training!--------\")\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, dev_loader, model, optimizer, scheduler, model_dir)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# start training\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, NER_config\u001b[38;5;241m.\u001b[39mepoch_num \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#开始验证\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     val_metrics \u001b[38;5;241m=\u001b[39m evaluate(dev_loader, model, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(train_loader, model, optimizer, scheduler, epoch)\u001b[0m\n\u001b[1;32m      8\u001b[0m batch_masks \u001b[38;5;241m=\u001b[39m batch_data\u001b[38;5;241m.\u001b[39mgt(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# get padding mask\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 计算损失值\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_token_starts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m             \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_labels\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     12\u001b[0m train_losses \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#梯度更新\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/analysis/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/analysis/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 42\u001b[0m, in \u001b[0;36mBertNER.forward\u001b[0;34m(self, input_data, token_type_ids, attention_mask, labels, position_ids, inputs_embeds, head_mask)\u001b[0m\n\u001b[1;32m     40\u001b[0m padded_sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(padded_sequence_output)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#将结果送入bilstm，再次提取特性        \u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m lstm_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbilstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded_sequence_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# 将lstm的结果送入线性层，进行五分类\u001b[39;00m\n\u001b[1;32m     44\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(lstm_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/analysis/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/analysis/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/analysis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:879\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    876\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    882\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    883\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[2097152, 1]' is invalid for input of size 1609728"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    run(NER_config)\n",
    "    test(NER_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77161477-f424-452b-bc16-8822be147c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7115/7115 [00:11<00:00, 605.62it/s]\n",
      "100%|██████████| 72/72 [00:00<00:00, 598.88it/s]\n"
     ]
    }
   ],
   "source": [
    " word_train, word_dev, label_train, label_dev = dev_split('train.npz')\n",
    "# 创建dataset\n",
    "train_dataset = NERDataset(word_train, label_train, NER_config)\n",
    "dev_dataset = NERDataset(word_dev, label_dev, NER_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e43da534-7b4a-46ee-b8f7-3942dd1c6972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0, 'B-entity': 1, 'I-entity': 2, 'E-entity': 3, 'S-entity': 4}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NER_config.label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c965081-9fec-4ca4-9819-66b9bfc7e133",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义推断函数\n",
    "def infer_function(dev_loader, model, mode='dev'):\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "    if mode == 'test':\n",
    "        tokenizer = BertTokenizer.from_pretrained(NER_config.robert_model, do_lower_case=True, skip_special_tokens=True)\n",
    "    id2label = NER_config.id2label\n",
    "    true_tags = []\n",
    "    pred_tags = []\n",
    "    sent_data = []\n",
    "    dev_losses = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, batch_samples in enumerate(dev_loader):\n",
    "            batch_data, batch_token_starts, batch_tags = batch_samples\n",
    "            if mode == 'test':\n",
    "                sent_data.extend([[tokenizer.convert_ids_to_tokens(idx.item()) for idx in indices\n",
    "                                   if (idx.item() > 0 and idx.item() != 101)] for indices in batch_data])\n",
    "            batch_masks = batch_data.gt(0)  # get padding mask, gt(x): get index greater than x\n",
    "            label_masks = batch_tags.gt(-1)  # get padding mask, gt(x): get index greater than x\n",
    "            # compute model output and loss\n",
    "            #loss = model((batch_data, batch_token_starts),\n",
    "                         #token_type_ids=None, attention_mask=batch_masks, labels=batch_tags)[0]\n",
    "            #dev_losses += loss.item()\n",
    "            # (batch_size, max_len, num_labels)\n",
    "            batch_output = model((batch_data, batch_token_starts),\n",
    "                                 token_type_ids=None, attention_mask=batch_masks)[0]\n",
    "            # (batch_size, max_len - padding_label_len)\n",
    "            batch_output = model.crf.decode(batch_output, mask=label_masks)\n",
    "            # (batch_size, max_len)\n",
    "            #batch_tags = batch_tags.to('cpu').numpy()\n",
    "            pred_tags.extend([[id2label.get(idx) for idx in indices] for indices in batch_output])\n",
    "    return pred_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a047901-5fb9-48d9-bf6e-f634d4c6fdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_infer(text):\n",
    "    words = list(text)\n",
    "    label = ['O'] * len(words)      \n",
    "    word_list = []\n",
    "    label_list = []\n",
    "    word_list.append(words)\n",
    "    label_list.append(label)\n",
    "    output_filename = '/home/zhenhengdong/WORk/NER/Try_ner/Datasets/Binary_file/infer.npz'\n",
    "    np.savez_compressed(output_filename,words = word_list, lables = label_list)\n",
    "    #重新加载\n",
    "    data = np.load(output_filename, allow_pickle=True)\n",
    "    word_test = data[\"words\"]\n",
    "    label_test = data[\"lables\"]\n",
    "    test_dataset = NERDataset(word_test, label_test, NER_config)\n",
    "    # build data_loader\n",
    "    test_loader = DataLoader(test_dataset, batch_size=NER_config.batch_size,\n",
    "                             shuffle=False, collate_fn=test_dataset.collate_fn)\n",
    "    # Prepare model\n",
    "    if NER_config.model_dir is not None:\n",
    "        #model = torch.load(NER_config.model_dir)\n",
    "        model = BertNER.from_pretrained(NER_config.model_dir)\n",
    "        model.to(NER_config.device)\n",
    "        logger.info(\"--------Load model from {}--------\".format(NER_config.model_dir))\n",
    "    else:\n",
    "        logger.info(\"--------No model to test !--------\")\n",
    "        return\n",
    "    pre_tegs = infer_function(test_loader, model, mode='test')\n",
    "    return pre_tegs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089addca-e9b0-4538-8a41-38f54933ba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '2022年11月，马拉西亚随荷兰国家队征战2022年卡塔尔世界杯'\n",
    "pre_tegs = new_infer(text)\n",
    " \n",
    "#取出位置\n",
    "start_index_list = []\n",
    "end_index_list = []\n",
    "for index in range(len(pre_tegs[0])):\n",
    "    if index != 0 and pre_tegs[0][index] !='O' and pre_tegs[0][index-1] == 'O':\n",
    "        start_index = index\n",
    "        start_index_list.append(start_index)\n",
    "    if  index != len(pre_tegs[0]) - 1 and pre_tegs[0][index] !='O' and pre_tegs[0][index+1] == 'O':\n",
    "        end_index = index\n",
    "        end_index_list.append(end_index)\n",
    "    if index == 0 and pre_tegs[0][index] !='O' :\n",
    "        start_index = index\n",
    "        start_index_list.append(start_index)\n",
    "    if  index == len(pre_tegs[0]) - 1 and  pre_tegs[0][index] !='O' :\n",
    "        end_index = index\n",
    "        end_index_list.append(end_index)\n",
    "#展示\n",
    "for index in range(len(start_index_list)):\n",
    "    print(text[start_index_list[index]:end_index_list[index]+1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
