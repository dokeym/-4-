{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d7d7fd9-ec17-4915-ae3d-18529088dc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/torch310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torchmetrics\n",
    "from corpus_proc import read_corpus, generate_dataloader\n",
    "from transformers import AdamW\n",
    "from ner_dataset import NerDataset\n",
    "from bert_bilstm_crf import BertCRF, BertBiLstmCRF\n",
    "\n",
    "from model_utils import custom_local_bert, custom_local_bert_tokenizer, load_ner_model, save_ner_model\n",
    "from config import get_parser\n",
    "from seqeval.metrics import accuracy_score\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de90f6db-57c2-4add-ac33-227a9a03c002",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "import warnings\n",
    "# 禁用UserWarning\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def get_dataloader(corpus_files, tags, tokenizer, batch_size=16):\n",
    "    \"\"\"\n",
    "    加载语料文件并通过转换为模型用dataloader\n",
    "    \"\"\"\n",
    "    sentences,sent_tags = read_corpus(corpus_files)\n",
    "    dataset = NerDataset(sentences, sent_tags)\n",
    "    data_loader = generate_dataloader(dataset, tokenizer, tags, batch_size)\n",
    "    return data_loader\n",
    "\n",
    "def train(opt, model, train_dl, test_dl):\n",
    "    \"\"\"\n",
    "    模型训练方法\n",
    "    \"\"\"\n",
    "    # 模型优化器\n",
    "    optimizer = AdamW(model.parameters(), lr=opt.learn_rate)\n",
    "\n",
    "    # training\n",
    "    for e in range(opt.epochs):\n",
    "        # evalute(opt, model, test_dl)\n",
    "\n",
    "        pbar = tqdm(train_dl)\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        # 在训练开始时创建一次\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "        for i, batch_data in enumerate(pbar):\n",
    "\n",
    "            # 模型输入\n",
    "            batch_data = { k:v.to(opt.device) for k,v in batch_data.items()}\n",
    "\n",
    "            if opt.use_amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    output = model(batch_data['input_ids'], batch_data['token_type_ids'], batch_data['attention_mask'])\n",
    "                    # 计算损失\n",
    "                    loss = model.loss(output, batch_data['label_ids'], batch_data['label_mask'])\n",
    "            else:\n",
    "                # logits\n",
    "                output = model(batch_data['input_ids'], batch_data['token_type_ids'], batch_data['attention_mask'])\n",
    "                # 计算损失\n",
    "                loss = model.loss(output, batch_data['label_ids'], batch_data['label_mask'])\n",
    "\n",
    "            if opt.use_amp:\n",
    "                pbar.set_description('Epochs %d/%d loss %f' % (e + 1, opt.epochs, loss.item()))\n",
    "\n",
    "                global train_loss_cnt\n",
    "                writer.add_scalar('train loss', loss.item(), train_loss_cnt)\n",
    "                train_loss_cnt += 1\n",
    "\n",
    "                # 缩放损失，然后调用backward()\n",
    "                # 来创建缩放后的梯度\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                # 消缩放梯度和调用\n",
    "                # 或跳过 optimizer.step()\n",
    "                scaler.step(optimizer)\n",
    "\n",
    "                # 为下一次迭代更新scaler\n",
    "                scaler.update()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "            else:\n",
    "                total_loss += loss * 1 / opt.accum_step\n",
    "\n",
    "                if i % opt.accum_step == opt.accum_step-1:\n",
    "                    # 计算模型参数梯度\n",
    "                    total_loss.backward()\n",
    "                    # 更新梯度\n",
    "                    optimizer.step()\n",
    "                    # 清除累计的梯度值\n",
    "                    model.zero_grad()\n",
    "\n",
    "                    pbar.set_description('Epochs %d/%d loss %f'%(e+1, opt.epochs, total_loss.item()))\n",
    "\n",
    "                    total_loss = 0\n",
    "\n",
    "        acc = evalute(opt, model, test_dl)\n",
    "        model.train()\n",
    "        # 每个epoch后保存模型\n",
    "        save_ner_model(opt, model, acc)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evalute(opt, model, test_dl):\n",
    "\n",
    "    predict_labels, target_labels = [],[]\n",
    "    model.eval()\n",
    "    pbar = tqdm(test_dl)\n",
    "    for batch_data in pbar:\n",
    "        batch_data = { k:v.to(opt.device) for k,v in batch_data.items()}\n",
    "        outputs = model(batch_data['input_ids'], batch_data['token_type_ids'], batch_data['attention_mask'])\n",
    "\n",
    "        # 解码\n",
    "        predicted = model.decode(outputs,batch_data['label_mask'])\n",
    "        pred_tags = [[opt.tags_rev[i] for i in sent[1:-1]] for sent in predicted]\n",
    "        mask = batch_data['label_mask']\n",
    "        tag_tags = [[opt.tags_rev[i.item()] for i in sent[mask[j]][1:-1]] for j,sent in enumerate(batch_data['label_ids'])]\n",
    "\n",
    "        predict_labels.extend(pred_tags)\n",
    "        target_labels.extend(tag_tags)\n",
    "\n",
    "        pbar.set_description('collect')\n",
    "\n",
    "\n",
    "    acc = accuracy_score(target_labels, predict_labels)\n",
    "\n",
    "    global eval_pr_cnt\n",
    "    targets_ = [opt.tags[t] for tags in target_labels for t in tags]\n",
    "    predicts_ = [opt.tags[t] for tags in predict_labels for t in tags]\n",
    "    writer.add_pr_curve('evaluate pr curve', torch.tensor(targets_), torch.tensor(predicts_), eval_pr_cnt)\n",
    "    eval_pr_cnt += 1\n",
    "\n",
    "    print(f'Accuracy of the model on evaluation: {acc * 100:.2f} %')\n",
    "    print(classification_report(target_labels, predict_labels))\n",
    "    return acc * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93d063c7-81a7-43b2-b70e-237ec9712d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 1/5 loss 17.012161:   9%|▊         | 499/5796 [01:12<12:00,  7.35it/s] Token indices sequence length is longer than the specified maximum sequence length for this model (565 > 512). Running this sequence through the model will result in indexing errors\n",
      "Epochs 1/5 loss 0.072937: 100%|██████████| 5796/5796 [15:29<00:00,  6.24it/s]  \n",
      "collect: 100%|██████████| 290/290 [01:13<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on evaluation: 99.13 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.92      0.94      0.93      3658\n",
      "         ORG       0.87      0.89      0.88      2185\n",
      "         PER       0.97      0.97      0.97      1864\n",
      "\n",
      "   micro avg       0.92      0.93      0.93      7707\n",
      "   macro avg       0.92      0.94      0.93      7707\n",
      "weighted avg       0.92      0.93      0.93      7707\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 2/5 loss 0.024597: 100%|██████████| 5796/5796 [16:13<00:00,  5.95it/s]  \n",
      "collect: 100%|██████████| 290/290 [01:12<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on evaluation: 99.26 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.94      0.95      0.94      3658\n",
      "         ORG       0.89      0.90      0.90      2185\n",
      "         PER       0.98      0.97      0.97      1864\n",
      "\n",
      "   micro avg       0.93      0.94      0.94      7707\n",
      "   macro avg       0.93      0.94      0.94      7707\n",
      "weighted avg       0.93      0.94      0.94      7707\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 3/5 loss 0.013184: 100%|██████████| 5796/5796 [16:34<00:00,  5.83it/s]  \n",
      "collect: 100%|██████████| 290/290 [01:15<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on evaluation: 99.32 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.95      0.96      0.95      3658\n",
      "         ORG       0.89      0.92      0.91      2185\n",
      "         PER       0.97      0.97      0.97      1864\n",
      "\n",
      "   micro avg       0.94      0.95      0.94      7707\n",
      "   macro avg       0.94      0.95      0.94      7707\n",
      "weighted avg       0.94      0.95      0.95      7707\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 4/5 loss 0.008881: 100%|██████████| 5796/5796 [16:39<00:00,  5.80it/s]  \n",
      "collect: 100%|██████████| 290/290 [01:17<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on evaluation: 99.27 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.94      0.95      0.95      3658\n",
      "         ORG       0.89      0.93      0.91      2185\n",
      "         PER       0.97      0.97      0.97      1864\n",
      "\n",
      "   micro avg       0.93      0.95      0.94      7707\n",
      "   macro avg       0.93      0.95      0.94      7707\n",
      "weighted avg       0.93      0.95      0.94      7707\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 5/5 loss 0.006805: 100%|██████████| 5796/5796 [16:43<00:00,  5.78it/s]  \n",
      "collect: 100%|██████████| 290/290 [01:14<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on evaluation: 99.32 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.95      0.96      0.95      3658\n",
      "         ORG       0.90      0.93      0.91      2185\n",
      "         PER       0.98      0.96      0.97      1864\n",
      "\n",
      "   micro avg       0.94      0.95      0.95      7707\n",
      "   macro avg       0.94      0.95      0.95      7707\n",
      "weighted avg       0.94      0.95      0.95      7707\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loss跟踪计数器\n",
    "train_loss_cnt = 0\n",
    "# pr评估计数器\n",
    "eval_pr_cnt = 0\n",
    "\n",
    "# 加载模型相关参数\n",
    "opt = get_parser()\n",
    "# 本地模型缓存目录\n",
    "local = os.path.abspath(os.path.join(opt.local_model_dir, opt.bert_model))\n",
    "# 加载定制bert模型\n",
    "bert_model = custom_local_bert(local, max_position=opt.max_position_length)\n",
    "tokenizer = custom_local_bert_tokenizer(local, max_position=opt.max_position_length)\n",
    "\n",
    "# 模型语料文件目录\n",
    "train_file = os.path.abspath(opt.train_file)\n",
    "dev_file = os.path.abspath(opt.dev_file)\n",
    "test_file = os.path.abspath(opt.test_file)\n",
    "\n",
    "# 模型训练语料\n",
    "train_dl = get_dataloader([train_file,dev_file], opt.tags, tokenizer, batch_size=opt.batch_size)\n",
    "test_dl = get_dataloader([test_file], opt.tags, tokenizer)\n",
    "\n",
    "# BiLSTMCRF模型\n",
    "model = BertBiLstmCRF(\n",
    "    bert_model=bert_model,\n",
    "    hidden_dim=opt.hidden_size,\n",
    "    target_size=len(opt.tags))\n",
    "\n",
    "# # 连续训练，加载之前存盘的模型\n",
    "# if os.path.exists(os.path.join(opt.save_model_dir, opt.load_model)):\n",
    "#     model = load_ner_model(opt, BertBiLstmCRF)\n",
    "\n",
    "model.to(opt.device)\n",
    "# 模型训练\n",
    "train(opt, model, train_dl, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e90a75-8281-49e1-9301-0d5c9cefe746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5334de35-d158-46ad-b24b-2f93fefd25f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8bb742-110d-4c93-a3cc-1d0ff8e08f31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch310",
   "language": "python",
   "name": "torch310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
